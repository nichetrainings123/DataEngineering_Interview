Project: Enterprise Data Modernization for Regulatory and Risk Analytics (Bank of America)

Project Overview :
Sure, one of the key projects I worked on at Bank of America was an Enterprise Data Modernization initiative where our main goal was to move critical regulatory and risk analytics data pipelines from legacy on-premise systems into a cloud-native architecture using AWS and Snowflake. The project supported multiple business domains like credit risk, customer onboarding, and transaction monitoring, and was part of a broader digital transformation program aimed at improving data accessibility, reducing latency, and meeting compliance requirements like HIPAA and GDPR.

Business Problem :
Previously, the data pipelines were running on Informatica and Teradata, and they were quite rigid and costly to maintain. Batch jobs ran overnight, often taking 6–8 hours, which delayed data availability for compliance and risk analysis teams. The existing setup also lacked flexibility in scaling up for month-end or regulatory reporting periods. So, our objective was to build a modern, automated, and event-driven data pipeline that could handle near real-time data ingestion, ensure security and auditability, and deliver faster insights to compliance officers and risk analysts.

My Role :
I was responsible for designing and implementing the end-to-end ETL/ELT pipeline using AWS Glue, Snowflake, and AWS Lambda. I also worked closely with the DevOps team to automate the infrastructure using CloudFormation templates and ensured the solution aligned with the bank’s internal security and compliance standards.

Technical Implementation :
Our data sources included transactional data from Oracle and SQL Server, customer profiles from on-prem systems, and unstructured data like logs and JSON feeds from S3 and Kafka.
We started by setting up AWS Glue jobs to extract data from S3 and JDBC connectors, apply transformations using PySpark, and then stage the cleansed data into Snowflake’s staging area.

Inside Snowflake, I designed the data model using a multi-layered architecture — staging, curated, and analytics layers. I implemented Clustering Keys and Materialized Views to improve query performance, and tuned our virtual warehouses for balanced compute utilization.

For near real-time ingestion, we implemented Snowpipe integrated with AWS Lambda triggers. Whenever a new file landed in S3, Lambda automatically invoked Snowpipe to load data into Snowflake tables. This eliminated manual triggers and reduced latency from hours to minutes.

To ensure governance and security, I configured Role-Based Access Control (RBAC) in Snowflake and used AWS KMS for data encryption. We also used AWS CloudWatch and ELK Stack to monitor job performance, track errors, and visualize operational metrics.

For job orchestration and dependency handling, we used AWS Step Functions to coordinate multiple Glue jobs and Snowflake SQL scripts. This provided a visual and fault-tolerant workflow that was easy to manage.

Performance Optimization:
One of the major improvements I worked on was optimizing query performance in Snowflake. By analyzing query profiles, I identified high-cost scans and redesigned a few tables using partition pruning and clustering keys. This reduced our query run time for compliance reports by around 40%.

I also replaced some heavy Python-based transformations with in-database Snowflake SQL transformations, which helped reduce compute load on Glue and made the pipelines faster and more cost-efficient.

Analytics and Reporting Layer:
For the reporting side, I collaborated with the BI team to build Power BI dashboards that connected directly to Snowflake using parameterized queries. These dashboards were designed for compliance and audit teams to monitor daily transaction volumes, exceptions, and threshold breaches. The key achievement here was enabling real-time visibility into regulatory metrics, which helped the bank’s compliance unit respond to anomalies within minutes instead of hours.

Collaboration and Governance:
Throughout the project, I worked closely with data scientists, governance teams, and AWS architects. I helped curate and publish certified datasets in Snowflake for advanced analytics and model training. We also implemented data lineage and metadata tracking using AWS Glue Data Catalog, which ensured full auditability — a key requirement in the banking domain.

I followed Agile methodology, participating in daily scrums, sprint planning, and reviews, where we continuously optimized the data flow based on feedback from business analysts and compliance users.

Outcome and Business Impact (end strong):
The new architecture reduced our end-to-end data processing time from about 6 hours to under 45 minutes for daily transaction feeds. The cost of infrastructure went down by approximately 20% after decommissioning legacy ETL servers, and the system became fully serverless and auto-scalable. Most importantly, we achieved real-time compliance monitoring, improved data quality, and simplified data access for regulatory audits and risk modeling.

This project really showcased the value of combining AWS serverless services with Snowflake’s powerful warehousing capabilities to build a modern, secure, and scalable data ecosystem for a highly regulated environment like banking.

✅ Key Technologies:
AWS Glue, Lambda, S3, Step Functions, CloudWatch, CloudFormation, Snowflake, Python (PySpark), SQL, Power BI, ELK Stack, Kafka, Teradata (legacy), Informatica (legacy), RBAC, HIPAA/GDPR Compliance.

##Short Note for above in single paragraph##

Sure. One of the key projects I worked on at Bank of America was part of a Data Modernization initiative where our goal was to migrate legacy Informatica and Teradata ETL workflows into a cloud-native architecture using AWS and Snowflake. The project supported regulatory and risk analytics teams, so performance, data quality, and compliance were all critical.

I was responsible for designing and developing the end-to-end data pipeline using AWS Glue, Lambda, and Snowflake. We ingested data from multiple sources — including Oracle, SQL Server, and S3 — and used Glue jobs with PySpark for cleaning, enrichment, and transformations. The processed data was then loaded into Snowflake’s staging and curated layers, where I optimized performance using Clustering Keys, partitioning, and SQL tuning.

For near real-time data ingestion, I integrated Snowpipe with AWS Lambda so that whenever a new file landed in S3, it automatically triggered data loads into Snowflake. This reduced data latency from hours to minutes. I also implemented RBAC in Snowflake and used AWS KMS encryption to ensure HIPAA and GDPR compliance.

We monitored and orchestrated the entire workflow through AWS Step Functions and CloudWatch, setting up alerts and dashboards for performance and error tracking. On the reporting side, I worked with the BI team to connect Power BI dashboards directly to Snowflake using parameterized queries, giving compliance officers real-time insights into transactions and anomalies.

Overall, the solution reduced processing time from about 6 hours to under 45 minutes, cut infrastructure costs by around 20%, and delivered a fully serverless, auto-scalable, and secure data pipeline. This project really strengthened my experience in combining AWS services and Snowflake to build modern, high-performance data platforms for large financial institutions.
