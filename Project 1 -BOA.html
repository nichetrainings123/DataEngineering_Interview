# Project: Enterprise Data Modernization for Regulatory and Risk Analytics (Bank of America)

## Project Overview
One of the key projects I worked on at Bank of America was an **Enterprise Data Modernization** initiative. The main goal was to migrate critical regulatory and risk analytics data pipelines from legacy on-premise systems into a **cloud-native architecture using AWS and Snowflake**.  
The project supported multiple business domains such as **credit risk, customer onboarding, and transaction monitoring**, and was part of a broader **digital transformation program** aimed at improving data accessibility, reducing latency, and meeting compliance requirements like **HIPAA and GDPR**.

---

## Business Problem
Previously, the data pipelines were running on **Informatica and Teradata**, and they were rigid and costly to maintain. Batch jobs ran overnight, often taking **6â€“8 hours**, which delayed data availability for compliance and risk analysis teams.  
The existing setup also lacked flexibility in scaling up during month-end or regulatory reporting periods.  

ðŸ‘‰ The objective was to build a **modern, automated, event-driven data pipeline** that could:
- Handle near real-time data ingestion  
- Ensure data security and auditability  
- Deliver faster insights to compliance officers and risk analysts  

---

## My Role
I was responsible for designing and implementing the **end-to-end ETL/ELT pipeline** using **AWS Glue, Snowflake, and AWS Lambda**.  
Additionally, I worked closely with the **DevOps team** to automate infrastructure using **AWS CloudFormation templates**, ensuring alignment with the bankâ€™s internal **security and compliance standards**.

---

## Technical Implementation
**Data Sources:**
- Transactional data from **Oracle** and **SQL Server**
- Customer profiles from on-prem systems
- Unstructured data (logs, JSON feeds) from **S3** and **Kafka**

**Data Pipeline Steps:**
1. Used **AWS Glue jobs** to extract data via S3 and JDBC connectors.  
2. Applied transformations using **PySpark**.  
3. Loaded cleansed data into **Snowflake staging area**.  

Inside Snowflake, I designed a **multi-layered architecture** â€” *staging, curated, and analytics layers*.  
- Implemented **Clustering Keys** and **Materialized Views** for query optimization.  
- Tuned **virtual warehouses** for balanced compute utilization.

**Real-Time Ingestion:**
- Implemented **Snowpipe** integrated with **AWS Lambda** triggers.  
- Whenever a file landed in S3, Lambda invoked Snowpipe to load data automatically.  
- Reduced latency from hours to **minutes**.

**Security & Governance:**
- Configured **Role-Based Access Control (RBAC)** in Snowflake.  
- Used **AWS KMS** for encryption.  
- Set up **CloudWatch** and **ELK Stack** for monitoring job performance and operational metrics.  

**Job Orchestration:**
- Leveraged **AWS Step Functions** to coordinate Glue jobs and Snowflake SQL scripts.  
- Achieved visual, fault-tolerant, and manageable workflows.

---

## Performance Optimization
One major improvement was optimizing **Snowflake query performance**.  
- Analyzed query profiles and redesigned high-cost scans with **partition pruning and clustering keys**.  
- Reduced compliance report runtime by **~40%**.  

Also replaced heavy Python transformations with **in-database SQL transformations**, reducing Glue compute load and improving cost efficiency.

---

## Analytics and Reporting Layer
Collaborated with the **BI team** to build **Power BI dashboards** connected directly to Snowflake via parameterized queries.  
- Dashboards monitored daily transactions, exceptions, and threshold breaches.  
- Enabled **real-time compliance visibility**, reducing response time from hours to **minutes**.

---

## Collaboration and Governance
Worked closely with **data scientists, governance teams, and AWS architects**.  
- Curated and published **certified datasets** in Snowflake for analytics and model training.  
- Implemented **data lineage and metadata tracking** using **AWS Glue Data Catalog**, ensuring auditability.  

Followed **Agile methodology** â€” daily scrums, sprint planning, and reviews to continuously optimize workflows based on business feedback.

---

## Outcome and Business Impact
ðŸš€ **Key Achievements:**
- Reduced data processing time from **6 hours â†’ under 45 minutes**  
- Cut infrastructure costs by **~20%** after legacy decommissioning  
- Achieved **real-time compliance monitoring** and improved **data quality**  
- Delivered **serverless, auto-scalable, and secure architecture**

---

## Key Technologies
**AWS Glue**, **Lambda**, **S3**, **Step Functions**, **CloudWatch**, **CloudFormation**,  
**Snowflake**, **Python (PySpark)**, **SQL**, **Power BI**, **ELK Stack**, **Kafka**,  
**Teradata (legacy)**, **Informatica (legacy)**, **RBAC**, **HIPAA/GDPR Compliance**.

