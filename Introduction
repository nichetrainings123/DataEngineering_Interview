Hi, thank you for having me today. I’m a Data Engineer with strong experience designing and building scalable data pipelines and analytics solutions, primarily on AWS and Snowflake. Over the last few years, I’ve focused on building end-to-end automated ETL and ELT pipelines that handle multi-terabyte datasets efficiently. Most of my work involves using AWS Glue, Lambda, and S3 to extract and transform large volumes of data, and then loading it into Snowflake for analytics.

In one of my recent projects, I built a fully automated data pipeline using AWS Glue and Snowflake, where data from multiple S3 sources was transformed and loaded into Snowflake. I used PySpark transformations for complex data cleansing and enrichment, and fine-tuned the SQL and clustering strategies inside Snowflake to significantly improve query performance. I also configured RBAC and implemented Clustering Keys in Snowflake, which helped reduce storage cost and improve performance while ensuring that our setup met HIPAA and internal compliance requirements.

I’ve also worked on event-driven data workflows using AWS Lambda and Snowpipe, where data transformations were triggered automatically as soon as files arrived in S3. That allowed near real-time updates to analytics dashboards and cut down processing delays substantially. In fact, the data we processed was being used directly in Power BI and SSRS reports, where I designed parameterized queries and visualizations to give stakeholders faster, more dynamic insights.

A big part of my career has also been about modernizing legacy data systems. I’ve migrated multiple Informatica ETL workflows into AWS Glue, replacing older, infrastructure-heavy pipelines with serverless, cost-efficient solutions that were easier to scale and maintain. This migration helped cut costs by around 20% and allowed us to handle growing data volumes much more effectively.

From an infrastructure standpoint, I’m very comfortable with AWS monitoring and logging tools like CloudWatch and ELK, which I’ve set up for resource monitoring, performance tracking, and alerting. I’ve also worked extensively with Python, Java, and shell scripting to automate and orchestrate data processing tasks.

Before focusing fully on AWS and Snowflake, I worked extensively in the Hadoop ecosystem—using MapReduce, HDFS, Hive, Spark, and Sqoop for large-scale data processing. I’ve built MapReduce jobs in Java, used Sqoop for incremental imports, Flume for collecting and storing web logs, and Spark SQL for high-performance transformations. I also integrated Cassandra and HBase with Hadoop and Storm for real-time analytics and data lookups. These experiences helped me develop a strong understanding of distributed computing and data architecture, which I’ve been able to apply effectively in modern cloud-based systems.

One project I really enjoyed was integrating AWS Glue with Snowflake in a hybrid environment — we combined Spark Streaming for real-time ingestion, AWS Step Functions for orchestration, and CloudFormation for automated infrastructure deployment. That gave us a flexible, fully automated data ecosystem that supported both batch and streaming workloads.

I’ve also collaborated closely with data scientists, helping them curate high-quality, consistent datasets in Snowflake for model training and validation. I believe strong data engineering directly impacts the accuracy and performance of ML models, so I always focus on data integrity, lineage, and governance.

Across all projects, my goal has been to design secure, scalable, and high-performing data architectures that drive business outcomes. Whether it’s optimizing query performance, automating workflows, or enabling real-time analytics, I enjoy building systems that make data more accessible and valuable to decision-makers.
